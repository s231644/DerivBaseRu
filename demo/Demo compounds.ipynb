{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "from src.Derivation import Derivation\n",
    "derivator = Derivation(use_guesser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.FinateStateMachine import FSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = dict()\n",
    "derivator.pos_all.append('part')\n",
    "for pos in derivator.pos_all:\n",
    "    wordlist_pos = list()\n",
    "    with open(f'../data/wiktionary/{pos}.txt', encoding='utf8') as f:\n",
    "        for l in f:\n",
    "            wordlist_pos.append(l.strip())\n",
    "    wordlist[pos] = wordlist_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, TimeoutError\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Part:\n",
    "    def __init__(self, name, pos_b, pos_a = None, rule_id = None, wordlist = []):\n",
    "        self.name = name\n",
    "        self.pos_b, self.pos_a = pos_b, pos_a or pos_b\n",
    "        self.rule_id = rule_id or name\n",
    "        print('FSM', self.rule_id)\n",
    "        self.fsm = FSM({self.rule_id})\n",
    "        if rule_id:        \n",
    "            with Pool(30) as p:\n",
    "                results = list(tqdm(p.imap(self.get_derived, wordlist), total=len(wordlist)))\n",
    "                print(len(results))\n",
    "                for result_, word in zip(results, wordlist):\n",
    "                    for result in result_:\n",
    "                        self.fsm.add_word(list(result.lower()) + [self.rule_id, (word, self.pos_b)])\n",
    "        else:\n",
    "            for word in tqdm(wordlist):\n",
    "                self.fsm.add_word(list(word.lower()) + [self.rule_id, (word, self.pos_b)])\n",
    "    \n",
    "    def get_derived(self, word):\n",
    "        derived = derivator.derive(word_b=word.lower(), pos_b=self.pos_b, rule_id=self.rule_id, use_rare=True)\n",
    "        if derived:\n",
    "            return derived[self.rule_id]\n",
    "        return []\n",
    "        \n",
    "    def add_word(self, form, lemma=None, pos=None):\n",
    "        self.fsm.add_word(list(form.lower()) + [self.rule_id, (lemma or form, pos or self.pos_b)])\n",
    "    \n",
    "    def analyze_word(self, word):\n",
    "        return self.fsm.analyze_word(word.lower())\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": self.name, \n",
    "            \"pos_b\": self.pos_b, \n",
    "            \"pos_a\": self.pos_a,\n",
    "            \"rule_id\": self.rule_id,\n",
    "            \"fsm\": self.fsm.to_dict()\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        c = cls(name=d[\"name\"], pos_b=d[\"pos_b\"], pos_a=d[\"pos_a\"], rule_id=d[\"rule_id\"])\n",
    "        c.fsm = FSM.from_dict(d[\"fsm\"])\n",
    "        return c\n",
    "    \n",
    "    def save(self):\n",
    "        with open(self.name.replace('/', '_') + \".pickle\", \"wb\") as f:\n",
    "            pickle.dump(self.to_dict(), f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, f):\n",
    "        d = pickle.load(open(f, \"rb\"))\n",
    "        return cls.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prt = Part(\"ruleINTERFIX(num)\", 'num', None, \"ruleINTERFIX(num)\", wordlist['num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prt.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prt.name + \".pickle\", \"rb\") as f:\n",
    "    prt0 = Part.load(prt.name + \".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prt0.fsm.analyze_word(\"двух\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_rare = dict()\n",
    "head_rare = dict()\n",
    "\n",
    "for pos in derivator.pos_all:\n",
    "    mod_rare[pos] = []\n",
    "    head_rare[pos] = []    \n",
    "    try:\n",
    "        with open(f'../src/rules/compounds_rare_{pos}.csv', encoding='utf8') as f:\n",
    "            for line in f.readlines()[1:]:\n",
    "                lemma, pos_lemma, form, i_arg = line.strip().split(';')    \n",
    "                if i_arg == '0':\n",
    "                    head_rare[pos].append((lemma, pos_lemma, form))\n",
    "                else:\n",
    "                    mod_rare[pos].append((lemma, pos_lemma, form))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    with open(f'../src/rules/compounds_rare_star.csv', encoding='utf8') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            lemma, pos_lemma, form, rule_id, i_arg = line.strip().split(';')    \n",
    "            if rule_id not in head_rare:\n",
    "                head_rare[rule_id] = []\n",
    "            if rule_id not in mod_rare:\n",
    "                mod_rare[rule_id] = []\n",
    "            if i_arg == '0':\n",
    "                head_rare[rule_id].append((lemma, pos_lemma, form))\n",
    "            else:\n",
    "                mod_rare[rule_id].append((lemma, pos_lemma, form))\n",
    "except FileNotFoundError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "head_parts = dict()\n",
    "mod_parts = dict()\n",
    "\n",
    "for rule in tqdm(derivator.rules_compound):\n",
    "    if rule.after_merge_rule_ids:\n",
    "        # not implemented yet\n",
    "        continue\n",
    "    head_rules, mod_rules = rule.simple_rule_ids[0], rule.simple_rule_ids[1]\n",
    "    if head_rules:\n",
    "        # e.g. suffix\n",
    "        head_rule = derivator.rules_dict[head_rules[0]]\n",
    "        if head_rule.pos_a == \"noun\":\n",
    "            continue\n",
    "        if os.path.exists(head_rule.name.replace('/', '_') + \".pickle\"):\n",
    "            head_parts[head_rule.name] = Part.load(head_rule.name.replace('/', '_') + \".pickle\")\n",
    "        elif head_rule.name not in head_parts:\n",
    "            head_parts[head_rule.name] = Part(\n",
    "                head_rule.name, \n",
    "                head_rule.pos_b, \n",
    "                head_rule.pos_a, \n",
    "                head_rule.name, \n",
    "                wordlist[head_rule.pos_b]\n",
    "            )\n",
    "            head_parts[head_rule.name].save()\n",
    "    else:\n",
    "        # no changes\n",
    "        if os.path.exists(rule.pos_b.replace('/', '_') + \".pickle\"):\n",
    "            head_parts[rule.pos_b] = Part.load(rule.pos_b.replace('/', '_') + \".pickle\")\n",
    "        elif rule.pos_b not in head_parts:\n",
    "            head_parts[rule.pos_b] = Part(rule.pos_b, rule.pos_b, rule.pos_b, None, wordlist[rule.pos_b])\n",
    "            for lemma, pos, form in head_rare.get(rule.pos_b, []):\n",
    "                head_parts[rule.pos_b].add_word(form, lemma, pos)\n",
    "            head_parts[rule.pos_b].save()\n",
    "    if mod_rules:\n",
    "        # interfix\n",
    "        mod_rule = derivator.rules_dict[mod_rules[0]]\n",
    "        if os.path.exists(mod_rule.name.replace('/', '_') + \".pickle\"):\n",
    "            mod_parts[mod_rule.name] = Part.load(mod_rule.name.replace('/', '_') + \".pickle\")\n",
    "        elif mod_rule.name not in mod_parts:\n",
    "            mod_parts[mod_rule.name] = Part(\n",
    "                mod_rule.name, \n",
    "                mod_rule.pos_b, \n",
    "                mod_rule.pos_a, \n",
    "                mod_rule.name, \n",
    "                wordlist[mod_rule.pos_b]\n",
    "            )\n",
    "            mod_parts[mod_rule.name].save()\n",
    "    else:\n",
    "        # no changes or star\n",
    "        if rule.poss_m[0]  == '*':\n",
    "            # star\n",
    "            if os.path.exists(rule.name.replace('/', '_') + \".pickle\"):\n",
    "                mod_parts[rule.name] = Part.load(rule.name.replace('/', '_') + \".pickle\")\n",
    "            elif rule.name not in mod_parts:\n",
    "                mod_parts[rule.name] = Part(rule.name, '*', '*', None, [])\n",
    "                for lemma, pos, form in mod_rare.get(rule.name, []):\n",
    "                    mod_parts[rule.name].add_word(form, lemma, pos)\n",
    "                mod_parts[rule.name].save()\n",
    "        else:\n",
    "            # adv, noun, etc.\n",
    "            pos_m = rule.poss_m[0]\n",
    "            if os.path.exists(pos_m.replace('/', '_') + \".pickle\"):\n",
    "                mod_parts[pos_m] = Part.load(pos_m.replace('/', '_') + \".pickle\")\n",
    "            elif pos_m not in mod_parts:\n",
    "                mod_parts[pos_m] = Part(pos_m, pos_m, pos_m, None, wordlist[pos_m])\n",
    "                for lemma, pos, form in mod_rare.get(pos_m, []):\n",
    "                    mod_parts[pos_m].add_word(form, lemma, pos)\n",
    "                mod_parts[pos_m].save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompoundAnalyzer:\n",
    "    def __init__(self, name, pos, left, right):\n",
    "        self.name = name\n",
    "        self.pos = pos\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def analyze(self, word, pos):\n",
    "        if pos != self.pos:\n",
    "            return []\n",
    "        left_res = self.left.analyze_word(word)\n",
    "        final_res = []\n",
    "        for st, left, left_rule in left_res:\n",
    "            if st == len(word):\n",
    "                continue\n",
    "            if word[st] == '-':\n",
    "                st += 1\n",
    "            right_res = self.right.analyze_word(word[st:])\n",
    "            for fi, right, right_rule in right_res:\n",
    "                if st + fi == len(word):\n",
    "                    final_res.append((word, self.pos, self.name, left, left_rule, right, right_rule))\n",
    "        return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed = []\n",
    "\n",
    "for rule in tqdm(derivator.rules_compound):\n",
    "#     if not rule.name.startswith(\"rule754\") and not rule.name.startswith(\"rule776\"):\n",
    "#         continue\n",
    "    if rule.after_merge_rule_ids:\n",
    "        # not implemented yet\n",
    "        continue\n",
    "    head_rules, mod_rules = rule.simple_rule_ids[0], rule.simple_rule_ids[1]\n",
    "    print(rule.name, head_rules, mod_rules)\n",
    "    if head_rules:\n",
    "        # e.g. suffix\n",
    "        head_rule = derivator.rules_dict[head_rules[0]]\n",
    "        head_part = head_parts[head_rule.name]\n",
    "    else:\n",
    "        # no changes\n",
    "        head_part = head_parts[rule.pos_b]\n",
    "    if mod_rules:\n",
    "        # interfix\n",
    "        mod_rule = derivator.rules_dict[mod_rules[0]]\n",
    "        mod_part = mod_parts[mod_rule.name]\n",
    "    else:\n",
    "        # no changes or star\n",
    "        if rule.poss_m[0]  == '*':\n",
    "            # star\n",
    "            mod_part = mod_parts[rule.name]\n",
    "        else:\n",
    "            # adv, noun, etc.\n",
    "            pos_m = rule.poss_m[0]\n",
    "            mod_part = mod_parts[pos_m]\n",
    "    \n",
    "    print(head_part, mod_part, rule.order)\n",
    "    if rule.order == [0, 1]:\n",
    "        ca = CompoundAnalyzer(rule.name, rule.pos_a, head_part, mod_part)\n",
    "    else:\n",
    "        # [1, 0]\n",
    "        ca = CompoundAnalyzer(rule.name, rule.pos_a, mod_part, head_part)\n",
    "    for word in wordlist[rule.pos_a]:\n",
    "        analyzed.extend(ca.analyze(word, rule.pos_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_parts['rule619*(noun + н1(ый) -> adj)'].fsm.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_comp_adj.txt', 'w') as f:\n",
    "    for l in analyzed:\n",
    "        w, p, nm, (ff, pf), iff, (s, ps), ss = l\n",
    "        f.writelines('\\t'.join([w, p, nm, ff, pf, iff, s, ps, ss]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = []\n",
    "for l in analyzed:\n",
    "    w, p, nm, (ff, pf), iff, (s, ps), ss = l\n",
    "    if w.find('-') != -1:\n",
    "        fixed.append(l)\n",
    "    else:\n",
    "        if len(ff) >= 3 and len(s) >= 3:\n",
    "            fixed.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('compo_new_cor3+.txt', 'w') as f:\n",
    "    for l in fixed:\n",
    "        w, p, nm, (ff, pf), iff, (s, ps), ss = l\n",
    "        f.writelines('\\t'.join([w, p, nm, ff, pf, iff, s, ps, ss]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed[::10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_used = set()\n",
    "for l in analyzed:\n",
    "    w, p, nm, (ff, pf), iff, (s, ps), ss = l\n",
    "    rules_used.add(nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = CompoundAnalyzer(derivator.rules_compound[52].name, derivator.rules_compound[52].pos_a, num_left, adj_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.analyze('двухметровый', 'adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed = []\n",
    "for word in tqdm(adj_wordlist):\n",
    "    analyzed.extend(ca.analyze(word, 'adj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(rule, word_b, pos_b, words_m, poss_m, pos_a, **kwargs):    \n",
    "    tags = [derivator.tag_guesser.guess(word=word_m, pos=pos_m) or [{}]\n",
    "            for (word_m, pos_m) in zip([word_b] + words_m, [pos_b] + poss_m)]\n",
    "    results = set()\n",
    "    for tag_comb in product(*tags):\n",
    "        results |= rule.apply_with_tags(word_b=word_b, pos_b=pos_b, pos_a=pos_a, words_m=words_m, poss_m=poss_m, tags_dict=tag_comb, **kwargs)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_words(rule, 'проходить', 'verb', ['первый'], ['adj'], 'noun', use_rare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
